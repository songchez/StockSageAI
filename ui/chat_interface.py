"""
ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ UI ëª¨ë“ˆ

ì‚¬ìš©ìì™€ AI ì–´ì‹œìŠ¤í„´íŠ¸ ê°„ì˜ ëŒ€í™” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""
import asyncio
import streamlit as st
from langchain_core.messages import HumanMessage
from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage
from langchain_teddynote.messages import astream_graph
from langchain_core.runnables import RunnableConfig

def render_chat_interface():
    """
    ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    """
    st.markdown("### ğŸ’¬ StockSage AIì™€ ëŒ€í™”")
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    display_chat_history()
    
    # ì œì•ˆ í”„ë¡¬í”„íŠ¸
    with st.expander("ğŸ’¡ ì´ëŸ° ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”", expanded=False):
        suggested_questions = [
            "ì˜¤ëŠ˜ íˆ¬ìí•˜ê¸° ê°€ì¥ ì¢‹ì€ ì£¼ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê¸°ìˆ  ì„¹í„°ì—ì„œ ì¶”ì²œ ì¢…ëª©ì´ ìˆë‚˜ìš”?",
            "í˜„ì¬ ì‹œì¥ ìƒí™©ì— ë§ëŠ” íˆ¬ì ì „ëµì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì• í”Œ(AAPL) ì£¼ì‹ì— ëŒ€í•œ ê¸°ìˆ ì  ë¶„ì„ì„ ë³´ì—¬ì£¼ì„¸ìš”.",
            "í…ŒìŠ¬ë¼(TSLA)ì˜ ì ì • ì§„ì…ì ê³¼ ëª©í‘œê°€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ì œ íˆ¬ì ì„±í–¥ì— ë§ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì„±í•´ì£¼ì„¸ìš”."
        ]
        
        col1, col2 = st.columns(2)
        for i, q in enumerate(suggested_questions):
            if i % 2 == 0:
                if col1.button(q, key=f"prompt_{i}", use_container_width=True):
                    # ì„¸ì…˜ ìƒíƒœì— í´ë¦­í•œ ì§ˆë¬¸ ì €ì¥
                    st.session_state.clicked_question = q
                    st.rerun()
            else:
                if col2.button(q, key=f"prompt_{i}", use_container_width=True):
                    # ì„¸ì…˜ ìƒíƒœì— í´ë¦­í•œ ì§ˆë¬¸ ì €ì¥
                    st.session_state.clicked_question = q
                    st.rerun()
    
    # í´ë¦­í•œ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì²˜ë¦¬
    if hasattr(st.session_state, 'clicked_question') and st.session_state.clicked_question:
        question = st.session_state.clicked_question
        st.session_state.clicked_question = None  # ì²˜ë¦¬ í›„ ì´ˆê¸°í™”
        
        if st.session_state.session_initialized:
            st.chat_message("user").markdown(question)
            with st.chat_message("assistant"):
                tool_placeholder = st.empty()
                text_placeholder = st.empty()
                resp, final_text, final_tool = st.session_state.event_loop.run_until_complete(
                    process_query(question, text_placeholder, tool_placeholder)
                )
            
            if "error" not in resp:
                st.session_state.history.append({"role": "user", "content": question})
                st.session_state.history.append(
                    {"role": "assistant", "content": final_text}
                )
                if final_tool.strip():
                    st.session_state.history.append(
                        {"role": "assistant_tool", "content": final_tool}
                    )
                st.rerun()
            else:
                st.error(resp["error"])

def display_chat_history():
    """
    ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    for message in st.session_state.history:
        if message["role"] == "user":
            st.chat_message("user").markdown(message["content"])
        elif message["role"] == "assistant":
            st.chat_message("assistant").markdown(message["content"])
        elif message["role"] == "assistant_tool":
            with st.chat_message("assistant").expander("ğŸ”§ ë¶„ì„ ì •ë³´", expanded=False):
                st.markdown(message["content"])

def get_streaming_callback(text_placeholder, tool_placeholder):
    """
    ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë§¤ê°œë³€ìˆ˜:
        text_placeholder: í…ìŠ¤íŠ¸ ì‘ë‹µì„ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        tool_placeholder: ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
    
    ë°˜í™˜ê°’:
        callback_func: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
        accumulated_text: ëˆ„ì ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
        accumulated_tool: ëˆ„ì ëœ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
    """
    accumulated_text = []
    accumulated_tool = []

    def callback_func(message: dict):
        nonlocal accumulated_text, accumulated_tool
        message_content = message.get("content", None)

        if isinstance(message_content, AIMessageChunk):
            content = message_content.content
            if isinstance(content, list) and len(content) > 0:
                message_chunk = content[0]
                if message_chunk["type"] == "text":
                    accumulated_text.append(message_chunk["text"])
                    text_placeholder.markdown("".join(accumulated_text))
                elif message_chunk["type"] == "tool_use":
                    if "partial_json" in message_chunk:
                        accumulated_tool.append(message_chunk["partial_json"])
                    else:
                        tool_call_chunks = message_content.tool_call_chunks
                        tool_call_chunk = tool_call_chunks[0]
                        accumulated_tool.append(
                            "\n```json\n" + str(tool_call_chunk) + "\n```\n"
                        )
                    with tool_placeholder.expander("ğŸ”§ ë¶„ì„ ì •ë³´", expanded=True):
                        st.markdown("".join(accumulated_tool))
        elif isinstance(message_content, ToolMessage):
            accumulated_tool.append(
                "\n```json\n" + str(message_content.content) + "\n```\n"
            )
            with tool_placeholder.expander("ğŸ”§ ë¶„ì„ ì •ë³´", expanded=True):
                st.markdown("".join(accumulated_tool))
        return None

    return callback_func, accumulated_text, accumulated_tool

async def process_query(query, text_placeholder, tool_placeholder, timeout_seconds=60):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ë§¤ê°œë³€ìˆ˜:
        query: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        text_placeholder: í…ìŠ¤íŠ¸ ì‘ë‹µì„ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        tool_placeholder: ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        timeout_seconds: ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ)
    
    ë°˜í™˜ê°’:
        response: ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ê°ì²´
        final_text: ìµœì¢… í…ìŠ¤íŠ¸ ì‘ë‹µ
        final_tool: ìµœì¢… ë„êµ¬ í˜¸ì¶œ ì •ë³´
    """
    try:
        if st.session_state.agent:
            streaming_callback, accumulated_text_obj, accumulated_tool_obj = (
                get_streaming_callback(text_placeholder, tool_placeholder)
            )
            try:
                response = await asyncio.wait_for(
                    astream_graph(
                        st.session_state.agent,
                        {"messages": [HumanMessage(content=query)]},
                        callback=streaming_callback,
                        config=RunnableConfig(
                            recursion_limit=100, thread_id=st.session_state.thread_id
                        ),
                    ),
                    timeout=timeout_seconds,
                )
            except asyncio.TimeoutError:
                error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                return {"error": error_msg}, error_msg, ""

            final_text = "".join(accumulated_text_obj)
            final_tool = "".join(accumulated_tool_obj)
            return response, final_text, final_tool
        else:
            return (
                {"error": "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "",
            )
    except Exception as e:
        import traceback

        error_msg = f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        return {"error": error_msg}, error_msg, ""